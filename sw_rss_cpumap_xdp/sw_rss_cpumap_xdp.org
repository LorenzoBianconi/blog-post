#+Title: Sw Receive Side Scaling with eBPF and CPUMaps

* Introduction

Modern Network Interface Cards (NICs) scale by having a hardware feature,
named Receive Side Scaling (RSS), that based on a flow-hash spread incoming
traffic across the RX irq-lines, which can be handled by different CPUs.
Unfortunately there can be a number of situations where the NIC hardware RSS
features fail, which result in delivering all packets to same RX IRQ-line
and thus same CPU.

This blogpost is about how to handle this situation in software, when RSS
fails, with a strong focus on how to solve this *using XDP and CPU-map*
redirect feature.

* Existing software receive steering

The Linux kernel already has a software feature called Receive Packet
Steering (RPS) and Receive Flow Steering (RFS), which is logically a
software implementation of RSS. This feature is both hard to configure [1]
and has limited scalability and performance.

The performance issue is because RPS and RFS, happen too late in the
kernels receive path, most importantly after the allocation of the "SKB"
metadata kernel object that keeps track of the packet. Transferring and
queuing these SKB-objects to a remote CPU is also a cross-CPU scalability
bottleneck that involves Inter Processor Communication calls and moving
cache-lines between CPUs. (Details: The kernels slab memory allocator is also
challenged as the per-CPU slab caches loose their effect).

* Faster software receive steering with XDP

A faster and more scaleable software solution is using XDP to redirect
raw-frames into a CPU-map. XDP is a kernel layer before the normal network
stack. This means it runs before allocating the mentioned SKB object, and
generally avoiding any per-packet memory allocations.

* What are XDP actions?

XDP runs an eBPF-program at the earliest possible point in the driver receive-path
when DMA rx-ring is synced for the CPU.

This eBPF-program parses the received frames and returns an action or verdict.
The actions are:
 1) XDP_DROP - drop frame, which at driver level means recycle without alloc.
 2) XDP_PASS - let it pass for normal network stack handling.
 3) XDP_TX - Bounce packet out same interface.
 4) XDP_REDIRECT - the advanced action this blogpost focus on.

Below is reported the Linux networking stack architecture, where we can notice the XDP
eBPF hook at the bottom of the stack.

[[file:images/XDP_arch.png]]

** What makes XDP_REDIRECT special?

The XDP_REDIRECT action is different, because it can queue XDP frame
(xdp_frame) objects into a BPF-map. All the other actions need to take
immediate action, because the (xdp_buff) data-structure that keeps track
packet-data is not-allocated anywhere, it is simply a variable on the
function call itself.

It is essential for performance to avoid per-packet allocations. When
XDP-redirecting the xdp_buff object is converted into a xdp_frame object to
allow queuing this object. To avoid any memory allocations, the xdp_frame
object is placed in the top headroom of the data-packet itself. (Details: A
CPU prefetch operation, runs before the XDP BPF-prog, which hides the
overhead of writing into this cache-line).

The XDP BPF-prog returns action XDP_REDIRECT, but prior to this it have
called one of these two BPF-helpers, to describe the redirect *destination*
where the frame should be redirected to:

- =bpf_redirect(ifindex, flags)=
- =bpf_redirect_map(bpf_map, index_key, flags)=

The first helper is simply choosing the Linux net device destination via the
ifindex as key. The second helper is the big leap that allow us to extend
XDP-redirect. This helper can redirect into a BPF-map with at a specific
index_key. As we will explore in the next section, this flexibility can be
used for CPU steering. As can be seen in the architecture picture, =AF_XDP=
is also implemented via =bpf_redirect_map=.

The ability to bulk is important for performance. The map-redirect is
responsible for creating a bulk effect, as drivers are required to call a
xdp_flush operation when NAPI-poll budget ends. The design allows the
individual map-type implementation to control the level of bulking. More
details later on how this is used to mitigate the overhead of cross-CPU
communication.

* Redirecting into a CPU-map

The BPF-maps concept is a generic key-value store with different types. It
is the main interface between a user-space application and a eBPF program
running in the kernel. The concept seems simple, but also very flexible as
it is up-to each individual BPF map-type to define the meaning of the key
and value. As of this writing there are 28 different map types[[[https://elixir.bootlin.com/linux/v5.10-rc2/source/include/uapi/linux/bpf.h#L130][2]]].

For our use-case of software Receive Side Scaling (RSS) with XDP, the CPUMAP
type (BPF_MAP_TYPE_CPUMAP) is just what we need. The CPUMAP represents the
CPUs in the system (zero) indexed as the map-key, and the map-value is the
config settings (per CPU map entry). Each CPUMAP entry has a dedicated
kernel thread bound to the given CPU in order to represent the remote-CPU
execution unit. (Hint: more about how we extended the map-value for adding
new features later in blogpost).

#+begin_src C
static int cpu_map_kthread_run(void *data) 
{ 
      /* do some work */ 
} 
 
int cpu_map_entry_alloc(int cpu, ...) 
{ 
      ... 
      rcpu->kthread = kthread_create_on_node(cpu_map_kthread_run, ...); 
      kthread_bind(rcpu->kthread, cpu); 
      wake_up_process(rcpu->kthread); 
      ... 
} 
#+end_src

We promised a faster solution with XDP, which is only possible due to the
careful design and bulking details happening internally in CPUMAP. This is
described in below section, which can be skipped based on readers interests.

** Details: Efficient transfer between CPUs

The CPUMAP entry represents a Multi-Producer Single-Consumer (MPSC) queue
(implemented via kernel provided ptr_ring). The Single-Consumer is the
kthread that can access the (ptr_ring) queue lockless. It also tries to bulk
dequeue 8 xdp-frame objects as this represent one cache-line. The
Multi-Producer's can be RX IRQ-line CPUs queuing up packets simultaneous for
the remote-CPU. To avoid queue lock-contention there is (per map-entry) a
small 8 object per-CPU allocated store for producer CPUs to generate bulk
enqueue into the cross-CPU queue. This careful queue usage means that each
cache-line moving cross-CPU transfer 8 frames.

** Moving raw-frames to remote-CPUs

The XDP eBPF-program attached to the NIC drivers RX-queue (connected to the
IRQ-line), will be the initial CPU that sees the packet. On this CPU the
BPF-prog should use as few CPU cycles as possible, to determine the
remote-CPU target, in-order to make this scale. This RX-CPU "redirect" the
received raw-packets to a given entry in the CPU map in order to move the
execution to the remote CPU associated to the map entry.

The remote-CPU map kthread will receive raw =xdp_frame= objects. Thus, the
SKB object allocation happens on the remote-CPU, and it is passed into to
the networking stack.

#+begin_src C
static int cpu_map_kthread_run(void *data)
{
      while (!kthread_should_stop()) {
            ...
            skb = cpu_map_build_skb();
            /* forward to the network stack */
            netif_receive_skb_core(skb);
            ...
      }
}
#+end_src

** Pitfall: Remote-CPU SKB creation lacking info

When creating an SKB based on the xdp_frame object, some optional SKB fields
are not updates. This is because these fields comes from the NIC hardware
RX-descriptor and on the remote-CPU this RX-descriptor is no-longer
available. The two most common hardware "partial-offload" information
missing is (1) HW RX-checksum info (=skb->ip_summed= + =skb->csum=), and (2)
HW RX-hash. Less commonly used (and also missing) is VLAN, RX-timestamping
and mark value.

The missing RX-checksum cause a slowdown when transmitting the SKB as the
checksum have to be recalculated. When network stack need to access/use the
hash value (see =skb_get_hash()=) it triggers a software recalculation of
the hash.

Currently CPUMAP don't call into the GRO system, that boost the TCP
throughput by creating an SKB that points to several TCP data-segments.
This is left as future work.

* New CPUMAP feature: Run XDP on remote CPU

Starting from Linux kernel version 5.9 [4] (and soon in RHEL8) the CPUMAP
can run a new (2nd) XDP program on the remote-CPU. This helps scalability as
the RX-CPU should spend as few cycles as possible per packet. Then the
remote-CPU that is scaled out to, can afford spend more cycles e.g. to look
deeper into packet headers.

#+begin_src C
static int cpu_map_bpf_prog_run_xdp(void *data)
{
      ...
      act = bpf_prog_run_xdp();
      switch (act) {
      case XDP_DROP:
         ...
      case XDP_PASS:
         ...
      case XDP_TX:
         ...
      case XDP_REDIRECT:
         ...
      }
      ...
}

static int cpu_map_kthread_run(void *data) {
      while (!kthread_should_stop()) {
            ...
            cpu_map_bpf_prog_run_xdp();
            ...
            skb = cpu_map_build_skb();
            /* forward to the network stack */
            netif_receive_skb_core(skb);
            ...
      }
}
#+end_src

This 2nd per remote-CPU XDP program is attached by inserting the eBPF
program (file-descriptor) on a map-entry level. This was achieved by
extending the map-value, now defined as UAPI via =struct bpf_cpumap_val=:

#+begin_src C
/* CPUMAP map-value layout
 *
 * The struct data-layout of map-value is a configuration interface.
 * New members can only be added to the end of this structure.
 */
struct bpf_cpumap_val {
	__u32 qsize;	/* queue size to remote target CPU */
	union {
		int   fd;	/* prog fd on map write */
		__u32 id;	/* prog id on map read */
	} bpf_prog;
};
#+end_src

* Practical use-cases

** Issue on low-end hardware

There are some multi-core devices available on the market (e.g. Marvell
EspressoBin [3]) that do not support RSS and all the interrupts generated by
the NIC are managed by a single cpu (e.g. cpu0)

However, using XDP and CPU maps is possible to implement a software
approximation of RSS.

Loading on the NIC a XDP program to redirect packets to CPU map entries, it is possible
to balance the traffic on all available cpus, executing just few instructions on the core
connected to the NIC irq-line. The eBPF program running on CPU map entries will implement
the logic to redirect the traffic to a remote interface or forward it to the networking stack.
Below is reported the system architecture run on the EspressoBin (mvneta).
We can notice most of the code is executed on the CPU map entry associated to cpu1

[[file:images/cpumap-test-arch.png]]


* Future development

In order to fill the gap with the "skb" scenario, we need to extend CPU maps
(and in general XDP) with JUMBO frames support and leverage on GRO code-path
available in the networking stack. No worries, we are already working on
it!!


* Additional Resources
- [1] https://www.kernel.org/doc/html/latest/networking/scaling.html
- [2] https://elixir.bootlin.com/linux/v5.10-rc2/source/include/uapi/linux/bpf.h#L130
- [3] http://espressobin.net/
- [4] https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9216477449f33cdbc9c9a99d49f500b7fbb81702">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9216477449f33cdbc9c9a99d49f500b7fbb81702

* Acronyms

Acronyms or Abbreviations:
- RSS = Receive Side Scaling
- RPS = Receive Packet Steering
- RFS = Receive Flow Steering
- XDP = eXpress Data Path
