eXpress Data Path (<strong>XDP</strong>) is an <strong>eBPF</strong> high performance data-plane available in the Linux kernel staring from version 4.8. The key concept of XDP is to parse the received frames just out of DMA rx ring avoiding per-packet memory overhead and allowing more effective bulking. Below is reported the Linux networking stack architecture, where we can notice the XDP eBPF hook at the bottom of the stack. A user-space application can attach an eBPF program to the XDP hook in order to implement packet filtering/mangling at line rate. Whenever the NIC receives a packet, the eBPF program attached to the XDP hook is triggered. After processing the frame, the XDP program will report to the NIC driver to forward the packet to the networking stack (<strong>XDP_PASS</strong>) or to a remote interface (<strong>XDP_REDIRECT</strong>), to drop it (<strong>XDP_DROP</strong>) or send it back to the same interface where the packet has been received (<strong>XDP_TX</strong>). Please take a look to [0] for a detailed introduction to XDP and eBPF.

&nbsp;

<img class="aligncenter size-full wp-image-812567" src="https://developers.redhat.com/blog/wp-content/uploads/2020/10/XDP_arch_scaled.png" alt="" width="645" height="350" data-wp-editing="1" />

In this article I will describe how it is possible to approximate the <strong>Receive Packet Steering</strong> (RPS) with XDP, eBPF and <strong>CPU maps</strong>.

<!--more-->
<h3>What are eBPF maps?</h3>
the main interface between a user-space application and a eBPF program running into the kernel are a "key, value" collection of data called <strong>maps</strong>. There are several type of maps available, like: hash maps, array maps, device maps, socket maps and <strong>CPU maps</strong>. Let's consider the latter ones.

CPU maps are data structure developed to represent host cpu architecture. It is possible to define a map with as many entries as the number of cpus available in the system. Each CPU map entry has a dedicated kernel thread binded to the given cpu in order to represent the execution unit.
<pre>static int cpu_map_kthread_run(void *data)
{
    /* do some work */
}

int cpu_map_entry_alloc(int cpu, ...)
{
    ...
    rcpu-&gt;kthread = kthread_create_on_node(cpu_map_kthread_run, ...);
    kthread_bind(rcpu-&gt;kthread, cpu);
    wake_up_process(rcpu-&gt;kthread);
    ...
}
</pre>
The XDP program attached to the NIC can "redirect" the received packets to a given entry in the CPU map in order to move the execution to the remote cpu associated to the map entry. The CPU map kthread will build the skb and forward it to the networking stack.
<pre>static int cpu_map_kthread_run(void *data)
{
    while (!kthread_should_stop()) {
        ...
        skb = cpu_map_build_skb();
        /* forward to the network stack */
        netif_receive_skb_core(skb);
        ...
    }
}
</pre>
<h3>Sw RPS with XDP and CPUMaps</h3>
There are some multi-core devices available on the market (e.g. Marvell EspressoBin [1]) that do not support hw RPS and all the interrupts generated by the NIC are managed by a single cpu (e.g. cpu0). However, using XDP and CPU maps is possible to implement a sw approximation of RPS. Starting from Linux kernel version 5.9 [3], CPU maps allow to attach an eBPF program to each entry in the map in order to XDP_TX, XDP_REDIRECT, XDP_DROP or XDP_PASS the received packet.
<pre>static int cpu_map_bpf_prog_run_xdp(void *data)
{
    ...
    act = bpf_prog_run_xdp();
    switch (act) {
    case XDP_DROP:
       ...
    case XDP_PASS:
       ...
    case XDP_TX:
       ...
    case XDP_REDIRECT:
       ...
    }
    ...
}

static int cpu_map_kthread_run(void *data) {
    while (!kthread_should_stop()) {
        ...
        cpu_map_bpf_prog_run_xdp();
        ...
        skb = cpu_map_build_skb();
        /* forward to the network stack */
        netif_receive_skb_core(skb);
        ...
    } 
}
</pre>
Loading on the NIC a XDP program to redirect packets to CPU map entries, it is possible to balance the traffic on all available cpus, executing just few instructions on the core connected to the NIC irq-line. The eBPF program running on CPU map entries will implement the logic to redirect the traffic to a remote interface or forward it to the networking stack. Below is reported the system architecture run on the EspressoBin (mvneta). We can notice most of the code is executed on the CPU map entry associated to cpu1

&nbsp;

<img class="aligncenter size-full wp-image-812397" src="https://developers.redhat.com/blog/wp-content/uploads/2020/10/cpumap-test-arch-1.png" alt="" width="968" height="394" />
<h3>Future development</h3>
In order to fill the gap with the "skb" scenario, we need to extend CPU maps (and in general XDP) with JUMBO frames support and leverage on <strong>GRO</strong> code-path available in the networking stack. No worries, we are already working on it!!
<h3>Additional Resources</h3>
<ul>
 	<li>[0] <a href="https://docs.cilium.io/en/latest/bpf/">https://docs.cilium.io/en/latest/bpf/</a></li>
 	<li>[1] <a href="http://espressobin.net/">http://espressobin.net/</a></li>
 	<li>[3] <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9216477449f33cdbc9c9a99d49f500b7fbb81702">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9216477449f33cdbc9c9a99d49f500b7fbb81702</a></li>
</ul>
