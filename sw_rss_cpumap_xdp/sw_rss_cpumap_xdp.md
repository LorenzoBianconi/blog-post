# Sw Receive Side Scaling with eBPF and CPUMaps

eXpress Data Path (**XDP**) is an **eBPF** high performance data-plane
available in the Linux kernel staring from version 4.8. The key concept
of XDP is to parse the received frames just out of DMA rx ring avoiding
per-packet memory overhead and allowing more effective bulking. Below is
reported the Linux networking stack architecture, where we can notice
the XDP eBPF hook at the bottom of the stack. A user-space application
can attach an eBPF program to the XDP hook in order to implement packet
filtering/mangling at line rate. Whenever the NIC receives a packet,
the eBPF program attached to the XDP hook is triggered. After processing
the frame, the XDP program will report to the NIC driver to forward the
packet to the networking stack (**XDP_PASS**) or to a remote interface
(**XDP_REDIRECT**), to drop it (**XDP_DROP**) or send it back to the
same interface where the packet has been received (**XDP_TX**).
Please take a look to [0] for a detailed introduction to XDP and eBPF.

<img class="aligncenter size-full wp-image-812567" src="https://developers.redhat.com/blog/wp-content/uploads/2020/10/XDP_arch_scaled.png" alt="" width="645" height="350" data-wp-editing="1" />

In this article I will describe how it is possible to approximate the
**Receive Side Scaling** (RSS) with XDP, eBPF and **CPU maps**.

### What are eBPF maps?
the main interface between a user-space application and a eBPF program
running into the kernel are a "key, value" collection of data called **maps**.
There are several type of maps available, like: hash maps, array maps,
device maps, socket maps and **CPU maps**. Let's consider the latter ones.

CPU maps are data structure developed to represent host cpu architecture.
It is possible to define a map with as many entries as the number of cpus
available in the system. Each CPU map entry has a dedicated kernel thread
binded to the given cpu in order to represent the execution unit.

	static int cpu_map_kthread_run(void *data) 
	{ 
	    /* do some work */ 
	} 
	 
	int cpu_map_entry_alloc(int cpu, ...) 
	{ 
	    ... 
	    rcpu-&gt;kthread = kthread_create_on_node(cpu_map_kthread_run, ...); 
	    kthread_bind(rcpu-&gt;kthread, cpu); 
	    wake_up_process(rcpu-&gt;kthread); 
	    ... 
	} 

The XDP program attached to the NIC can "redirect" the received packets to a
given entry in the CPU map in order to move the execution to the remote cpu
associated to the map entry. The CPU map kthread will build the skb and
forward it to the networking stack.

	static int cpu_map_kthread_run(void *data)
	{
	    while (!kthread_should_stop()) {
	        ...
	        skb = cpu_map_build_skb();
	        /* forward to the network stack */
	        netif_receive_skb_core(skb);
	        ...
	    }
	}

### Sw RSS with XDP and CPUMaps
There are some multi-core devices available on the market (e.g. Marvell EspressoBin [1])
that do not support hw RSS and all the interrupts generated by the NIC are managed by
a single cpu (e.g. cpu0). However, using XDP and CPU maps is possible to implement a sw
approximation of RSS. Starting from Linux kernel version 5.9 [3], CPU maps allow to attach
an eBPF program to each entry in the map in order to XDP_TX, XDP_REDIRECT, XDP_DROP or
XDP_PASS the received packet.

	static int cpu_map_bpf_prog_run_xdp(void *data)
	{
	    ...
	    act = bpf_prog_run_xdp();
	    switch (act) {
	    case XDP_DROP:
	       ...
	    case XDP_PASS:
	       ...
	    case XDP_TX:
	       ...
	    case XDP_REDIRECT:
	       ...
	    }
	    ...
	}
	
	static int cpu_map_kthread_run(void *data) {
	    while (!kthread_should_stop()) {
	        ...
	        cpu_map_bpf_prog_run_xdp();
	        ...
	        skb = cpu_map_build_skb();
	        /* forward to the network stack */
	        netif_receive_skb_core(skb);
	        ...
	    } 
	}

Loading on the NIC a XDP program to redirect packets to CPU map entries, it is possible
to balance the traffic on all available cpus, executing just few instructions on the core
connected to the NIC irq-line. The eBPF program running on CPU map entries will implement
the logic to redirect the traffic to a remote interface or forward it to the networking stack.
Below is reported the system architecture run on the EspressoBin (mvneta).
We can notice most of the code is executed on the CPU map entry associated to cpu1

<img class="aligncenter size-full wp-image-812397" src="https://developers.redhat.com/blog/wp-content/uploads/2020/10/cpumap-test-arch-1.png" alt="" width="968" height="394" />

### Future development
In order to fill the gap with the "skb" scenario, we need to extend CPU maps (and in general XDP)
with JUMBO frames support and leverage on **GRO** code-path available in the networking stack.
No worries, we are already working on it!!

### Additional Resources
<ul>
 	<li>[0] <a href="https://docs.cilium.io/en/latest/bpf/">https://docs.cilium.io/en/latest/bpf/</a></li>
 	<li>[1] <a href="http://espressobin.net/">http://espressobin.net/</a></li>
 	<li>[3] <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9216477449f33cdbc9c9a99d49f500b7fbb81702">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9216477449f33cdbc9c9a99d49f500b7fbb81702</a></li>
</ul>
